# -*- coding: utf-8 -*-
"""
Created on Sun Dec 10 15:00:23 2017

@author: chenfa
"""

import requests
import urllib
import math
import copy
import pandas as pd
import os
from bs4 import BeautifulSoup
import numpy as np


from requests_kerberos import HTTPKerberosAuth, OPTIONAL
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait


cert = r"C:\Users\chenfa\Documents\YardLog\cacerts.pem"
os.environ["REQUESTS_CA_BUNDLE"] = cert
kerberos = HTTPKerberosAuth(mutual_authentication=OPTIONAL)

#driver =webdriver.Edge(r'C:\Users\chenfa\Documents\webdriver\MicrosoftWebDriver')
driver =webdriver.Chrome(r'C:\Users\chenfa\Documents\webdriver\chromedriver')
#driver = webdriver.Firefox()
url = 'https://capacity.amazon.com/sccp/101/storage_requirement_metric'
driver.get(url)

driver.find_element_by_id('show_real_time_inventory_checkbox').click()
driver.find_element_by_xpath("//input[@value ='Table']").click()
#driver.find_element_by_xpath("//input[@value ='Excel']").click()
html = (driver.execute_script("return document.getElementsByTagName('html')[0].innerHTML"))
#soup = BeautifulSoup(html, 'lxml')
#webscrape from webpage


###updated read_html 

#def __init__(self, html):
#    self.html = html
##   self.url      = url
##   self.r        = requests.get(self.url)
#    self.html_soup = BeautifulSoup(self.html,'lxml')

        
def read(html):
    html_soup = BeautifulSoup(html,'lxml')
    tables      = []
    table_div = html_soup.find('div', {'id': 'container'})
    #table = table_div.findAll("table")[]
    tables_html = table_div.find_all("table")[2]
    
    # Parse each table
    for n in range(0, len(tables_html)):
        
        n_cols = 0
        n_rows = 0
        
        for row in tables_html[n].find_all("tr"):
            col_tags = row.find_all(["td", "th"])
            if len(col_tags) > 0:
                n_rows += 1
                if len(col_tags) > n_cols:
                    n_cols = len(col_tags)
        
        # Create dataframe
        df = pd.DataFrame(index = range(0, n_rows), columns = range(0, n_cols))
        
			# Create list to store rowspan values 
        skip_index = [0 for i in range(0, n_cols)]
			
        # Start by iterating over each row in this table...
        row_counter = 0
        for row in tables_html[n].find_all("tr"):
            
            # Skip row if it's blank
            if len(row.find_all(["td", "th"])) == 0:
                next
            
            else:
                
                # Get all cells containing data in this row
                columns = row.find_all(["td", "th"])
                col_dim = []
                row_dim = []
                col_dim_counter = -1
                row_dim_counter = -1
                col_counter = -1
                this_skip_index = copy.deepcopy(skip_index)
                
                for col in columns:
                    
                    # Determine cell dimensions
                    colspan = col.get("colspan")
                    if colspan is None:
                        col_dim.append(1)
                    else:
                        col_dim.append(int(colspan))
                    col_dim_counter += 1
                        
                    rowspan = col.get("rowspan")
                    if rowspan is None:
                        row_dim.append(1)
                    else:
                        row_dim.append(int(rowspan))
                    row_dim_counter += 1
                        
                    # Adjust column counter
                    if col_counter == -1:
                        col_counter = 0  
                    else:
                        col_counter = col_counter + col_dim[col_dim_counter - 1]
                        
                    while skip_index[col_counter] > 0:
                        col_counter += 1

                    # Get cell contents  
                    cell_data = col.get_text()
                    
                    # Insert data into cell
                    df.iat[row_counter, col_counter] = cell_data

                    # Record column skipping index
                    if row_dim[row_dim_counter] > 1:
                        this_skip_index[col_counter] = row_dim[row_dim_counter]
            
            # Adjust row counter 
            row_counter += 1
            
            # Adjust column skipping index
            skip_index = [i - 1 if i > 0 else i for i in this_skip_index]

        # Append dataframe to list of tables
        tables.append(df)
    
    return(tables)

x = read(html)

url_soup = BeautifulSoup(html, "lxml")

#table_div = soup.find('div', {'id': 'container'})
#table = table_div.findAll("table")[1]
#df = pd.read_html(table.prettify())
#df = pd.DataFrame(df[0])
#df.to_csv(r'C:\Users\chenfa\Documents\YardLog\cu.csv', index = False )
