# -*- coding: utf-8 -*-
"""
Created on Sun Dec 10 15:00:23 2017

@author: chenfa
"""

#import requests
#import urllib
#import math
import copy
import pandas as pd
#import os
from bs4 import BeautifulSoup
#import numpy as np
#lazy 

#from requests_kerberos import HTTPKerberosAuth, OPTIONAL
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By


#webscrape from webpage
###updated read_html 

#def __init__(self, html):
#    self.html = html
##   self.url      = url
##   self.r        = requests.get(self.url)
#    self.html_soup = BeautifulSoup(self.html,'lxml')
        
def read(html):
    html_soup = BeautifulSoup(html,'lxml')
    tables      = []
    table_div = html_soup.find('div', {'id': 'container'})
    #table = table_div.findAll("table")[]
    tables_html = table_div.find_all("table", {'class': 'class name'})
    # Parse each table
    for n in range(0, len(tables_html)):
        
        n_cols = 0
        n_rows = 0
        
        for row in tables_html[n].find_all("tr"):
            col_tags = row.find_all(["td", "th"])
            if len(col_tags) > 0:
                n_rows += 1
                if len(col_tags) > n_cols:
                    n_cols = len(col_tags)
        
        # Create dataframe
        df = pd.DataFrame(index = range(0, n_rows), columns = range(0, n_cols))
        
			# Create list to store rowspan values 
        skip_index = [0 for i in range(0, n_cols)]
			
        # Start by iterating over each row in this table...
        row_counter = 0
        for row in tables_html[n].find_all("tr"):
            
            # Skip row if it's blank
            if len(row.find_all(["td", "th"])) == 0:
                next
            
            else:
                
                # Get all cells containing data in this row
                columns = row.find_all(["td", "th"])
                col_dim = []
                row_dim = []
                col_dim_counter = -1
                row_dim_counter = -1
                col_counter = -1
                this_skip_index = copy.deepcopy(skip_index)
                
                for col in columns:
                    
                    # Determine cell dimensions
                    colspan = col.get("colspan")
                    if colspan is None:
                        col_dim.append(1)
                    else:
                        col_dim.append(int(colspan))
                    col_dim_counter += 1
                        
                    rowspan = col.get("rowspan")
                    if rowspan is None:
                        row_dim.append(1)
                    else:
                        row_dim.append(int(rowspan))
                    row_dim_counter += 1
                        
                    # Adjust column counter
                    if col_counter == -1:
                        col_counter = 0  
                    else:
                        col_counter = col_counter + col_dim[col_dim_counter - 1]
                        
                    while skip_index[col_counter] > 0:
                        col_counter += 1

                    # Get cell contents  
                    cell_data = col.get_text()
                    
                    # Insert data into cell
                    df.iat[row_counter, col_counter] = cell_data

                    # Record column skipping index
                    if row_dim[row_dim_counter] > 1:
                        this_skip_index[col_counter] = row_dim[row_dim_counter]
            
            # Adjust row counter 
            row_counter += 1
            
            # Adjust column skipping index
            skip_index = [i - 1 if i > 0 else i for i in this_skip_index]

        # Append dataframe to list of tables
        tables.append(df)
    
    return(tables)



 #CU  
driver =webdriver.Chrome(r'C:\Users\chromedriver')

url = 'https:/storage_requirement_metric'
driver.get(url)
wait = WebDriverWait(driver, 15)
wait.until(EC.element_to_be_clickable((By.ID, "show_real_time_inventory_checkbox"))).click()
#driver.find_element_by_id('show_real_time_inventory_checkbox').click()
driver.find_element_by_xpath("//input[@value ='Table']").click()


WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, "CPMetricTablePANTRY")))
html = (driver.execute_script("return document.getElementsByTagName('html')[0].innerHTML"))

#driver.find_element_by_xpath("//input[@value ='Excel']").click()

#soup = BeautifulSoup(html, 'lxml')

x = read(html)

df = pd.concat(x)
    
df.to_csv(r'C:\\cu.csv', index = False )


#CUCA
url = 'https://storage_requirement_metric'
driver.get(url)
wait = WebDriverWait(driver, 15)
wait.until(EC.element_to_be_clickable((By.ID, "show_real_time_inventory_checkbox"))).click()
#driver.find_element_by_id('show_real_time_inventory_checkbox').click()
driver.find_element_by_xpath("//input[@value ='Table']").click()
#driver.find_element_by_xpath("//input[@value ='Excel']").click()
WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, "CPMetricTableNONSORTABLE")))
html = (driver.execute_script("return document.getElementsByTagName('html')[0].innerHTML"))



x = read(html)

df = pd.concat(x)
    
df.to_csv(r'C:cuca.csv', index = False )

#BG

#import os

url = 'https://view_transpose?org=US&report_id='
driver.get(url)
WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.ID, "inbound/inbound-view_transpose")))


#webscrape from webpage

html = (driver.execute_script("return document.getElementsByTagName('html')[0].innerHTML"))
soup = BeautifulSoup(html, 'lxml')
table= soup.find('table', {'class': 'table table-condensed table-hover table-report'})

for body in table('tbody'):
    body.unwrap()
    
df = pd.read_html(str(table), flavor = "bs4")
df = pd.DataFrame(df[0])
df.to_csv(r'C:bg.csv', index = False )


#gbCA
url = 'https:/view_transpose?org=CA&report_id='
driver.get(url)

#webscrape from webpage
WebDriverWait(driver, 20).until(EC.visibility_of_all_elements_located((By.ID, "inbound/inbound-view_transpose")))
html = (driver.execute_script("return document.getElementsByTagName('html')[0].innerHTML"))
driver.quit()
    
soup = BeautifulSoup(html, 'lxml')
table= soup.find('table', {'class': 'table table-condensed table-hover table-report'})

for body in table('tbody'):
    body.unwrap()
    
df = pd.read_html(str(table), flavor = "bs4")
df = pd.DataFrame(df[0])
df.to_csv(r'C:\\bgca.csv', index = False )
